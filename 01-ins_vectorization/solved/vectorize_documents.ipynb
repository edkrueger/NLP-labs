{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with SKL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Documents (without scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the scikit-learn, is great',\n",
       " 'much better API for; the NLP than the spark MLlib',\n",
       " 'we are+ learning NLP in the scikit-learn',\n",
       " 'is my... punctuation, is. terrible;']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make some documents to work with\n",
    "\n",
    "X = [\n",
    "    'the scikit-learn, is great',\n",
    "    'much better API for; the NLP than the spark MLlib',\n",
    "    'we are+ learning NLP in the scikit-learn',\n",
    "    'is my... punctuation, is. terrible;'\n",
    "]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile scikit-learn like formatter regex\n",
    "\n",
    "formatter_pattern = re.compile(r'[^\\w\\s\\']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the scikitlearn is great',\n",
       " 'much better api for the nlp than the spark mllib',\n",
       " 'we are learning nlp in the scikitlearn',\n",
       " 'is my punctuation is terrible']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show scikit-learn like formatter in action\n",
    "\n",
    "X_formatted = [re.sub(formatter_pattern, '', document).lower() for document in X]\n",
    "X_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Formatted Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile scikit-learn like tokenizer regex\n",
    "\n",
    "tokenizer_pattern = re.compile(r'(?u)\\b\\w\\w+\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'scikitlearn', 'is', 'great'],\n",
       " ['much',\n",
       "  'better',\n",
       "  'api',\n",
       "  'for',\n",
       "  'the',\n",
       "  'nlp',\n",
       "  'than',\n",
       "  'the',\n",
       "  'spark',\n",
       "  'mllib'],\n",
       " ['we', 'are', 'learning', 'nlp', 'in', 'the', 'scikitlearn'],\n",
       " ['is', 'my', 'punctuation', 'is', 'terrible']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn like show tokenization of formatted documents\n",
    "\n",
    "X_tokenized = [tokenizer_pattern.findall(document) for document in X_formatted]\n",
    "X_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash tokenized documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to hash documents\n",
    "\n",
    "def hash_tokenized(tokenized_documents):\n",
    "    used_token = []\n",
    "    vocabulary = {}\n",
    "    idx = 0\n",
    "\n",
    "    hashed_docuements = []\n",
    "\n",
    "    for document in tokenized_documents:\n",
    "\n",
    "        hashed_document = []\n",
    "\n",
    "        for token in document:\n",
    "\n",
    "            if token in vocabulary:\n",
    "                hashed_value = vocabulary[token]\n",
    "            else:\n",
    "                hashed_value = idx\n",
    "                idx += 1\n",
    "                vocabulary[token] = hashed_value\n",
    "\n",
    "            hashed_document.append(hashed_value)\n",
    "\n",
    "        hashed_docuements.append(hashed_document)\n",
    "        \n",
    "    max_idx = idx - 1\n",
    "\n",
    "    return hashed_docuements, vocabulary, max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash the tokenized documents\n",
    "X_hashed, hashing_vocabulary, max_idx = hash_tokenized(X_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3],\n",
       " [4, 5, 6, 7, 0, 8, 9, 0, 10, 11],\n",
       " [12, 13, 14, 8, 15, 0, 1],\n",
       " [2, 16, 17, 2, 18]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'scikitlearn': 1,\n",
       " 'is': 2,\n",
       " 'great': 3,\n",
       " 'much': 4,\n",
       " 'better': 5,\n",
       " 'api': 6,\n",
       " 'for': 7,\n",
       " 'nlp': 8,\n",
       " 'than': 9,\n",
       " 'spark': 10,\n",
       " 'mllib': 11,\n",
       " 'we': 12,\n",
       " 'are': 13,\n",
       " 'learning': 14,\n",
       " 'in': 15,\n",
       " 'my': 16,\n",
       " 'punctuation': 17,\n",
       " 'terrible': 18}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashing_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert hashed documents to maxtix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashed_to_matrix(hashed_documents, max_idx):\n",
    "    \n",
    "    matrix = []\n",
    "    \n",
    "    for hashed_document in hashed_documents:\n",
    "        \n",
    "        row = [0 for _ in range(0, max_idx + 1)]\n",
    "        \n",
    "        for hashed_token in hashed_document:\n",
    "            \n",
    "            row[hashed_token] += 1\n",
    "        \n",
    "        matrix.append(row)\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
       " [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed_to_matrix(X_hashed, max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'scikitlearn': 1,\n",
       " 'is': 2,\n",
       " 'great': 3,\n",
       " 'much': 4,\n",
       " 'better': 5,\n",
       " 'api': 6,\n",
       " 'for': 7,\n",
       " 'nlp': 8,\n",
       " 'than': 9,\n",
       " 'spark': 10,\n",
       " 'mllib': 11,\n",
       " 'we': 12,\n",
       " 'are': 13,\n",
       " 'learning': 14,\n",
       " 'in': 15,\n",
       " 'my': 16,\n",
       " 'punctuation': 17,\n",
       " 'terrible': 18}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashing_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertorize Docuements (w scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the CountVectorizer class\n",
    "\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit it\n",
    "\n",
    "cv.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 18,\n",
       " 'scikit': 14,\n",
       " 'learn': 7,\n",
       " 'is': 6,\n",
       " 'great': 4,\n",
       " 'much': 10,\n",
       " 'better': 2,\n",
       " 'api': 0,\n",
       " 'for': 3,\n",
       " 'nlp': 12,\n",
       " 'than': 17,\n",
       " 'spark': 15,\n",
       " 'mllib': 9,\n",
       " 'we': 19,\n",
       " 'are': 1,\n",
       " 'learning': 8,\n",
       " 'in': 5,\n",
       " 'my': 11,\n",
       " 'punctuation': 13,\n",
       " 'terrible': 16}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the vocabulary\n",
    "\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 18)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t2\n",
      "  (2, 1)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 18)\t1\n",
      "  (2, 19)\t1\n",
      "  (3, 6)\t2\n",
      "  (3, 11)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 16)\t1\n"
     ]
    }
   ],
   "source": [
    "# see the sparse matrix\n",
    "\n",
    "X_sparse = cv.transform(X)\n",
    "print(X_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "        [1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 2, 0],\n",
       "        [0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the dense matrix\n",
    "\n",
    "X_dense = X_sparse.todense()\n",
    "X_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertorize Docuements Ignoring Stop Words (w scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'when', 'becoming', 'was', 'out', 'anyhow', 'ie', 'has', 'throughout', 'because', 'between', 'together', 'former', 'ours', 'so', 'while', 'she', 'third', 'each', 'ltd', 'become', 'will', 'seems', 'own', 'whither', 'we', 'who', 'someone', 'besides', 'three', 'made', 'see', 'herself', 'anything', 'since', 'in', 'off', 'noone', 'us', 'couldnt', 'cant', 'keep', 'them', 'neither', 'down', 'before', 'amoungst', 'wherein', 'myself', 'anywhere', 'very', 'next', 'five', 'least', 'therein', 'describe', 'serious', 'name', 'further', 'what', 'behind', 'if', 'thin', 'my', 'sometimes', 'inc', 'anyway', 'fill', 'hundred', 'where', 'how', 'con', 'among', 'eleven', 'seem', 'eg', 'thereby', 'of', 'well', 'de', 'often', 'never', 'forty', 'along', 'can', 'her', 'without', 'otherwise', 'two', 'therefore', 'somewhere', 'about', 'however', 'top', 'such', 'against', 'rather', 're', 'mostly', 'your', 'above', 'most', 'our', 'by', 'that', 'mine', 'moreover', 'system', 'beforehand', 'sometime', 'been', 'his', 'across', 'might', 'yourself', 'with', 'sincere', 'up', 'which', 'alone', 'there', 'thick', 'those', 'call', 'i', 'please', 'yourselves', 'became', 'whatever', 'thru', 'co', 'hereafter', 'everyone', 'namely', 'now', 'you', 'either', 'to', 'else', 'afterwards', 'show', 'empty', 'fifteen', 'upon', 'whoever', 'due', 'part', 'for', 'fire', 'back', 'he', 'may', 'below', 'un', 'should', 'do', 'although', 'but', 'they', 'from', 'get', 'nor', 'put', 'take', 'beside', 'several', 'whereby', 'herein', 'then', 'latterly', 'around', 'full', 'anyone', 'except', 'nothing', 'almost', 'could', 'within', 'yours', 'am', 'none', 'after', 'until', 'it', 'side', 'found', 'hence', 'sixty', 'also', 'me', 'indeed', 'interest', 'many', 'ten', 'toward', 'thus', 'whence', 'find', 'still', 'hereby', 'more', 'themselves', 'being', 'per', 'four', 'thereupon', 'twelve', 'mill', 'much', 'nobody', 'these', 'even', 'ourselves', 'whereas', 'front', 'give', 'cannot', 'nevertheless', 'were', 'one', 'every', 'than', 'here', 'nowhere', 'an', 'last', 'or', 'under', 'first', 'why', 'hasnt', 'be', 'twenty', 'nine', 'have', 'any', 'though', 'through', 'seemed', 'bottom', 'another', 'as', 'on', 'meanwhile', 'whereupon', 'whole', 'amongst', 'already', 'too', 'into', 'thereafter', 'go', 'and', 'everywhere', 'whom', 'done', 'all', 'itself', 'detail', 'via', 'whose', 'would', 'a', 'himself', 'once', 'some', 'both', 'seeming', 'must', 'ever', 'enough', 'whether', 'other', 'during', 'beyond', 'others', 'thence', 'only', 'its', 'move', 'had', 'towards', 'hers', 'again', 'whenever', 'their', 'six', 'whereafter', 'wherever', 'onto', 'him', 'etc', 'something', 'hereupon', 'is', 'everything', 'at', 'formerly', 'this', 'are', 'cry', 'bill', 'same', 'perhaps', 'fifty', 'somehow', 'over', 'becomes', 'elsewhere', 'less', 'eight', 'amount', 'the', 'no', 'yet', 'always', 'latter', 'few', 'not'})\n"
     ]
    }
   ],
   "source": [
    "# see the english stopwords\n",
    "\n",
    "from sklearn.feature_extraction import stop_words \n",
    "print(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the CountVectorizer class, have it ignore the english stop words\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit it\n",
    "\n",
    "cv.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scikit': 8,\n",
       " 'learn': 3,\n",
       " 'great': 2,\n",
       " 'better': 1,\n",
       " 'api': 0,\n",
       " 'nlp': 6,\n",
       " 'spark': 9,\n",
       " 'mllib': 5,\n",
       " 'learning': 4,\n",
       " 'punctuation': 7,\n",
       " 'terrible': 10}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the vocabulary\n",
    "\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 9)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 8)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 10)\t1\n"
     ]
    }
   ],
   "source": [
    "# see the sparse matrix\n",
    "\n",
    "X_sparse = cv.transform(X)\n",
    "print(X_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the dense matrix\n",
    "\n",
    "X_dense = X_sparse.todense()\n",
    "X_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertorize Docuements with TF-IDF Ignoring Stop Words (w scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the TfidfVectorizer class, have it ignore the english stop words\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.5264054336099155\n",
      "  (0, 3)\t0.5264054336099155\n",
      "  (0, 2)\t0.6676785446095399\n",
      "  (1, 1)\t0.4651619335222394\n",
      "  (1, 0)\t0.4651619335222394\n",
      "  (1, 6)\t0.3667390112974172\n",
      "  (1, 9)\t0.4651619335222394\n",
      "  (1, 5)\t0.4651619335222394\n",
      "  (2, 8)\t0.4658085493691629\n",
      "  (2, 3)\t0.4658085493691629\n",
      "  (2, 6)\t0.4658085493691629\n",
      "  (2, 4)\t0.5908190806023349\n",
      "  (3, 7)\t0.7071067811865476\n",
      "  (3, 10)\t0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "# do fit and transform at the same time and show the sparse matrix\n",
    "\n",
    "X_sparse = tfidf.fit_transform(X)\n",
    "print(X_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scikit': 8,\n",
       " 'learn': 3,\n",
       " 'great': 2,\n",
       " 'better': 1,\n",
       " 'api': 0,\n",
       " 'nlp': 6,\n",
       " 'spark': 9,\n",
       " 'mllib': 5,\n",
       " 'learning': 4,\n",
       " 'punctuation': 7,\n",
       " 'terrible': 10}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the vocabulary\n",
    "\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.66767854, 0.52640543, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.52640543, 0.        ,\n",
       "         0.        ],\n",
       "        [0.46516193, 0.46516193, 0.        , 0.        , 0.        ,\n",
       "         0.46516193, 0.36673901, 0.        , 0.        , 0.46516193,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.46580855, 0.59081908,\n",
       "         0.        , 0.46580855, 0.        , 0.46580855, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.70710678, 0.        , 0.        ,\n",
       "         0.70710678]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the dense matrix\n",
    "\n",
    "X_dense = X_sparse.todense()\n",
    "X_dense"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
